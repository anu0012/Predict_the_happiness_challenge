{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "f3fa79a9-3350-46a9-995c-16c7c9792840",
    "_uuid": "09c13dcbcf37d42b7ddff577ebca12ebe3c5048c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.classify import SklearnClassifier\n",
    "#from wordcloud import WordCloud,STOPWORDS\n",
    "#import xgboost as xgb\n",
    "np.random.seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "92831422-0dc4-4278-bce6-a497c41da0a9",
    "_uuid": "ce6631f5a3daf80925cc8ba32773fbe955de7ebc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "9ab34b6b-2731-4884-914c-81e2ed792ea7",
    "_uuid": "cc1a3dc4180aaddc41fc75c8048799fc630a6039",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping_target = {'happy':0, 'not happy':1}\n",
    "train = train.replace({'Is_Response':mapping_target})\n",
    "\n",
    "# Browser Mapping\n",
    "mapping_browser = {'Firefox':0, 'Mozilla':0, 'Mozilla Firefox':0,\n",
    "                  'Edge': 1, 'Internet Explorer': 1 , 'InternetExplorer': 1, 'IE':1,\n",
    "                   'Google Chrome':2, 'Chrome':2,\n",
    "                   'Safari': 3, 'Opera': 4\n",
    "                  }\n",
    "train = train.replace({'Browser_Used':mapping_browser})\n",
    "test = test.replace({'Browser_Used':mapping_browser})\n",
    "# Device mapping\n",
    "mapping_device = {'Desktop':0, 'Mobile':1, 'Tablet':2}\n",
    "train = train.replace({'Device_Used':mapping_device})\n",
    "test = test.replace({'Device_Used':mapping_device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "7c079219-1497-4519-9cf1-37451ce0300b",
    "_uuid": "e01bd7c899e2da37ebcb2651909e97b3d3a27bda",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = test['User_ID']\n",
    "target = train['Is_Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "544cc63c-c8c4-4378-9899-006fc9caac11",
    "_uuid": "7a8db0dce6abd3f2191911e09aeac01b7dea8602",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean data\n",
    "import string\n",
    "import itertools \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "stops = ['also','on','the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This','january','february','march','april','may','june','july','august','september','october',\n",
    "        'november','december','monday','tuesday','wednesday','thursday','friday','saturday','sunday','india','tripadviser','usa',\n",
    "        'hundred','thousand','today','tomorrow','yesterday','etc','delhi','mumbai','chennai','kolkata','room','hotel','even',\n",
    "        'front desk','new york','san francisco','however','time square','canada','review','us','uk','china','staff','found'\n",
    "        ,'one','area','although','walking', 'distance','though','th','floor','really','got','people','lobby','location']\n",
    "# punct = list(string.punctuation)\n",
    "# punct.append(\"''\")\n",
    "# punct.append(\":\")\n",
    "# punct.append(\"...\")\n",
    "# punct.append(\"@\")\n",
    "# punct.append('\"\"')\n",
    "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n",
    "    txt = str(text)\n",
    "    \n",
    "    # Replace apostrophes with standard lexicons\n",
    "    txt = txt.replace(\"isn't\", \"is not\")\n",
    "    txt = txt.replace(\"aren't\", \"are not\")\n",
    "    txt = txt.replace(\"ain't\", \"am not\")\n",
    "    txt = txt.replace(\"won't\", \"will not\")\n",
    "    txt = txt.replace(\"didn't\", \"did not\")\n",
    "    txt = txt.replace(\"shan't\", \"shall not\")\n",
    "    txt = txt.replace(\"haven't\", \"have not\")\n",
    "    txt = txt.replace(\"hadn't\", \"had not\")\n",
    "    txt = txt.replace(\"hasn't\", \"has not\")\n",
    "    txt = txt.replace(\"don't\", \"do not\")\n",
    "    txt = txt.replace(\"wasn't\", \"was not\")\n",
    "    txt = txt.replace(\"weren't\", \"were not\")\n",
    "    txt = txt.replace(\"doesn't\", \"does not\")\n",
    "    txt = txt.replace(\"'s\", \" is\")\n",
    "    txt = txt.replace(\"'re\", \" are\")\n",
    "    txt = txt.replace(\"'m\", \" am\")\n",
    "    txt = txt.replace(\"'d\", \" would\")\n",
    "    txt = txt.replace(\"'ll\", \" will\")\n",
    "    txt = txt.replace(\"--th\", \" \")\n",
    "    \n",
    "    # More cleaning\n",
    "    txt = re.sub(r\"alot\", \"a lot\", txt)\n",
    "    txt = re.sub(r\"what's\", \"\", txt)\n",
    "    txt = re.sub(r\"What's\", \"\", txt)\n",
    "    txt = re.sub(r\"\\'s\", \" \", txt)\n",
    "    txt = txt.replace(\"pic\", \"picture\")\n",
    "    txt = re.sub(r\"\\'ve\", \" have \", txt)\n",
    "    txt = re.sub(r\"can't\", \"cannot \", txt)\n",
    "    txt = re.sub(r\"n't\", \" not \", txt)\n",
    "    txt = re.sub(r\"I'm\", \"I am\", txt)\n",
    "    txt = re.sub(r\" m \", \" am \", txt)\n",
    "    txt = re.sub(r\"\\'re\", \" are \", txt)\n",
    "    txt = re.sub(r\"\\'d\", \" would \", txt)\n",
    "    txt = re.sub(r\"\\'ll\", \" will \", txt)\n",
    "    txt = re.sub(r\"60k\", \" 60000 \", txt)\n",
    "    txt = re.sub(r\" e g \", \" eg \", txt)\n",
    "    txt = re.sub(r\" b g \", \" bg \", txt)\n",
    "    txt = re.sub(r\"\\0s\", \"0\", txt)\n",
    "    txt = re.sub(r\" 9 11 \", \"911\", txt)\n",
    "    txt = re.sub(r\"e-mail\", \"email\", txt)\n",
    "    txt = re.sub(r\"\\s{2,}\", \" \", txt)\n",
    "    txt = re.sub(r\"quikly\", \"quickly\", txt)\n",
    "    txt = re.sub(r\"imrovement\", \"improvement\", txt)\n",
    "    txt = re.sub(r\"intially\", \"initially\", txt)\n",
    "    txt = re.sub(r\"quora\", \"Quora\", txt)\n",
    "    txt = re.sub(r\" dms \", \"direct messages \", txt)  \n",
    "    txt = re.sub(r\"demonitization\", \"demonetization\", txt) \n",
    "    txt = re.sub(r\"actived\", \"active\", txt)\n",
    "    txt = re.sub(r\"kms\", \" kilometers \", txt)\n",
    "    txt = re.sub(r\"KMs\", \" kilometers \", txt)\n",
    "    txt = re.sub(r\" cs \", \" computer science \", txt) \n",
    "    txt = re.sub(r\" upvotes \", \" up votes \", txt)\n",
    "    txt = re.sub(r\" iPhone \", \" phone \", txt)\n",
    "    txt = re.sub(r\"\\0rs \", \" rs \", txt) \n",
    "    txt = re.sub(r\"calender\", \"calendar\", txt)\n",
    "    txt = re.sub(r\"ios\", \"operating system\", txt)\n",
    "    txt = re.sub(r\"gps\", \"GPS\", txt)\n",
    "    txt = re.sub(r\"gst\", \"GST\", txt)\n",
    "    txt = re.sub(r\"programing\", \"programming\", txt)\n",
    "    txt = re.sub(r\"bestfriend\", \"best friend\", txt)\n",
    "    txt = re.sub(r\"dna\", \"DNA\", txt)\n",
    "    txt = re.sub(r\"III\", \"3\", txt) \n",
    "    txt = re.sub(r\"the US\", \"America\", txt)\n",
    "    txt = re.sub(r\"Astrology\", \"astrology\", txt)\n",
    "    txt = re.sub(r\"Method\", \"method\", txt)\n",
    "    txt = re.sub(r\"Find\", \"find\", txt) \n",
    "    txt = re.sub(r\"banglore\", \"Banglore\", txt)\n",
    "    txt = re.sub(r\" J K \", \" JK \", txt)\n",
    "    txt = re.sub(r\"comfy\", \"comfortable\", txt)\n",
    "    txt = re.sub(r\"colour\", \"color\", txt)\n",
    "    txt = re.sub(r\"travellers\", \"travelers\", txt)\n",
    "\n",
    "    # Emoji replacement\n",
    "#     txt = re.sub(r':\\)',r' Happy ',txt)\n",
    "#     txt = re.sub(r':D',r' Happy ',txt)\n",
    "#     txt = re.sub(r':P',r' Happy ',txt)\n",
    "#     txt = re.sub(r':\\(',r' Sad ',txt)\n",
    "    \n",
    "    # Remove urls and emails\n",
    "    txt = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', txt, flags=re.MULTILINE)\n",
    "    txt = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', txt, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    txt = ''.join([c for c in text if c not in punctuation])\n",
    "#     txt = txt.replace(\".\", \" \")\n",
    "#     txt = txt.replace(\":\", \" \")\n",
    "#     txt = txt.replace(\"!\", \" \")\n",
    "#     txt = txt.replace(\"&\", \" \")\n",
    "#     txt = txt.replace(\"#\", \" \")\n",
    "    \n",
    "    # Remove all symbols\n",
    "    txt = re.sub(r'[^A-Za-z0-9\\s]',r' ',txt)\n",
    "    txt = re.sub(r'\\n',r' ',txt)\n",
    "    \n",
    "    txt = re.sub(r'[0-9]',r' ',txt)\n",
    "    \n",
    "    # Replace words like sooooooo with so\n",
    "    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n",
    "    \n",
    "    # Split attached words\n",
    "    #txt = \" \".join(re.findall('[A-Z][^A-Z]*', txt))   \n",
    "    \n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stops])\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "#         print (len(txt.split()))\n",
    "#         print (txt)\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "    \n",
    "    if lemmatization:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_emoji = [':-)', ':)', ':-]', ':]', ':-3', ':3', ':->', ':>', '8-)', '8)', ':-}', ':}', ':o)', ':c)', ':^)', '=]', '=)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=D', '=3', 'B^D', ':-))', ';-)', ';)', '*-)', '*)', ';-]', ';]', ';^)', ';D', ':-P', ':P', 'X-P', 'x-p', ':-p', ':p', ':-?', ':?', ':-?', ':?', ':-b', ':b', '=p', '>:P', ':*', ':-*', '^.^', '^_^', '^-^', 'xd']\n",
    "neg_emoji = [':-(', ':(', ':-c', ':c', ':-<', ':<', ':-[', ':[', ':-||', '>:[', ':{', ':@', '>:(', ':-/', ':/', '>:\\\\', '>:/', ':\\\\', '=/', '=\\\\', ':L', '=L', ':S', ':-|', ':|', ':-X', ':X', '-.-', '-,-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Number of positive emoji in the text ##\n",
    "train[\"number_of_positive_emo\"] = train['Description'].apply(lambda x: len([c for c in str(x).split() if c in pos_emoji]))\n",
    "test[\"number_of_positive_emo\"] = test['Description'].apply(lambda x: len([c for c in str(x).split() if c in pos_emoji]))\n",
    "\n",
    "## Number of negative emoji in the text ##\n",
    "train[\"number_of_negative_emo\"] = train['Description'].apply(lambda x: len([c for c in str(x).split() if c in neg_emoji]))\n",
    "test[\"number_of_negative_emo\"] = test['Description'].apply(lambda x: len([c for c in str(x).split() if c in neg_emoji]))\n",
    "\n",
    "\n",
    "train[\"num_words\"] = train[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "test[\"num_words\"] = test[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train[\"num_unique_words\"] = train[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_unique_words\"] = test[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train[\"num_chars\"] = train[\"Description\"].apply(lambda x: len(str(x)))\n",
    "test[\"num_chars\"] = test[\"Description\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train[\"num_stopwords\"] = train[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n",
    "test[\"num_stopwords\"] = test[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "train[\"num_punctuations\"] =train['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_punctuations\"] =test['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train[\"num_words_upper\"] = train[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test[\"num_words_upper\"] = test[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train[\"num_words_title\"] = train[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test[\"num_words_title\"] = test[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train[\"mean_word_len\"] = train[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"mean_word_len\"] = test[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "d9dc880e-c6f3-4057-afd0-0a4dc63a10ea",
    "_uuid": "90911e4c16ea8148206eda7cffbeb413292e1f4c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean description\n",
    "train['Description'] = train['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = True))\n",
    "test['Description'] = test['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "c3552c263cb682e9ce9eda4fa45070814192a691",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Is_Response'] = np.nan\n",
    "alldata = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "62f6c755-9732-4ce5-a48a-ddbd2ff79e01",
    "_uuid": "af1a7a898517000bc344755bf2c18fc7995fa44e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,4),min_df = 100,max_df=0.8, max_features = 8000, sublinear_tf=True,\n",
    "                             use_idf=True)\n",
    "tfidfdata = tfidfvec.fit_transform(alldata['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "1117cdcb-76cc-4531-bd55-de269e75629b",
    "_uuid": "bc01300b8f5fb499fc0b89ad2e8622b37aaf4c3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataframe for features\n",
    "tfidf_df = pd.DataFrame(tfidfdata.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "85ee1dc9-73ca-4321-836b-ca2cf3ff9f58",
    "_uuid": "4addab62fcc75d4bd5e23900916f9e8947fc383c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "0cc15622f3e38120a4abc26368d62bec6ac3bf38",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfid_df_train = tfidf_df[:len(train)]\n",
    "tfid_df_test = tfidf_df[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "7e994804-e238-4d95-b881-ec626a1e9897",
    "_uuid": "af4f93a445fc4dd949b7f7ab6c84127ea79467ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the merged data file into train and test respectively\n",
    "train_feats = alldata[~pd.isnull(alldata.Is_Response)]\n",
    "test_feats = alldata[pd.isnull(alldata.Is_Response)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "ae52ab9a-7add-48d2-933c-45eb6e46f854",
    "_uuid": "f3571a0d3da337faff7cc507b41ee4b5dc950ae5",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "### set target variable\n",
    "\n",
    "train_feats['Is_Response'] = [1 if x == 'happy' else 0 for x in train_feats['Is_Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "f04d7900-7ea8-4db5-9d01-a5021af58ce1",
    "_uuid": "7508e84c1f9dd2a5ab7c215db415ffeb9bf1594d",
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3ce4f6e5be2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'number_of_positive_emo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'number_of_negative_emo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Browser_Used'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Device_Used'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_unique_words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_chars'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_stopwords'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_punctuations'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_words_upper'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_words_title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean_word_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#cols = ['Browser_Used','Device_Used']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_feats2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfid_df_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_feats2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfid_df_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                        copy=copy)\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    406\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   4830\u001b[0m     blocks = [make_block(\n\u001b[1;32m   4831\u001b[0m         \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4832\u001b[0;31m         placement=placement) for placement, join_units in concat_plan]\n\u001b[0m\u001b[1;32m   4833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4834\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4830\u001b[0m     blocks = [make_block(\n\u001b[1;32m   4831\u001b[0m         \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4832\u001b[0;31m         placement=placement) for placement, join_units in concat_plan]\n\u001b[0m\u001b[1;32m   4833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4834\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.4/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   4943\u001b[0m         \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4945\u001b[0;31m             \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4946\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4947\u001b[0m         \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# merge into a new data frame with tf-idf features\n",
    "cols = ['number_of_positive_emo','number_of_negative_emo','Browser_Used','Device_Used','num_words','num_unique_words','num_chars','num_stopwords','num_punctuations','num_words_upper','num_words_title','mean_word_len']\n",
    "#cols = ['Browser_Used','Device_Used']\n",
    "train_feats2 = pd.concat([train_feats[cols], tfid_df_train], axis=1)\n",
    "test_feats2 = pd.concat([test_feats[cols], tfid_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l1', dual=False, tol=1e-8, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=None)\n",
    "clf2 = LogisticRegression(penalty='l2', dual=False, tol=1e-8, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=None)\n",
    "#clf3 = LogisticRegression(penalty='l2', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=0.2, class_weight=None, random_state=25)\n",
    "#clf3 = BernoulliNB()\n",
    "model = VotingClassifier(estimators=[('lr', clf1), ('lr1', clf2)],weights=[3,3], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd798b10-8436-4aaa-964f-1ec7934d2850",
    "_uuid": "99efe1b02067e79726a6644422a550ca705b510e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Naive Bayes 2 - tfidf is giving higher CV score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "print(cross_val_score(model, train_feats2, target, cv=5, scoring=make_scorer(accuracy_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a28ca5a8-6efe-48fe-8921-26a18dff55df",
    "_uuid": "50ac7a4ecaef4ab0a2eb30ce6a3ebec521ca9ae5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_feats2, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test_feats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ffdf85a8-7d83-402b-be9c-cb8abc50d7e1",
    "_uuid": "a3ada5bfdbc3f8ad00925402e1ae6ea0a6b1aca3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['User_ID'] = test_id\n",
    "result['Is_Response'] = preds\n",
    "mapping = {0:'happy', 1:'not_happy'}\n",
    "result = result.replace({'Is_Response':mapping})\n",
    "\n",
    "result.to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b81dfd4-cc2b-480b-bdd7-dcd403f7ca77",
    "_uuid": "57d4e28385f699cf9d4d37d9b2473e310f204bfd",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
